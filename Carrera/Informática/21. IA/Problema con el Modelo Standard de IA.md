El modelo standard asume que al agente se le proveé un objetivo completamente especificado.
Estos objetivos específicos son más difíciles de definir cuando los problemas son menos artificiales (por ejemplo, ganar una partida de ajedrez) y provienen del mundo real. 
El problema resultante se llama el [[Problema de alineamiento de Valores]]. 
Como no queremos una máquina que sea inteligente para perseguir *sus* objetivos, sino los nuestros, si estos objetivos no se pueden expresar completamente, entonces el modelo es, a la larga, inadecuado.
***
